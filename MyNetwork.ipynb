{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "joined-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faced-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    \n",
    "    def __init__(self, _X, _Y, _batch_size = None, shuffle = True):\n",
    "        assert len(_X) == len(_Y)\n",
    "        self.X = _X\n",
    "        self.Y = _Y\n",
    "        self.batch_size = _batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def shuffle(self):\n",
    "        indices = np.arange(len(self.X))\n",
    "        np.random.shuffle(indices)\n",
    "        self.X = np.array([self.X[i] for i in indices])\n",
    "        self.Y = np.array([self.Y[i] for i in indices])\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.X, self.Y\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        if self.shuffle: self.shuffle()\n",
    "        \n",
    "        if self.batch_size == None:\n",
    "            yield (np.matrix(self.X), np.matrix(self.Y))\n",
    "            return\n",
    "            \n",
    "        for i in range(0, len(self.X), self.batch_size):\n",
    "            yield (np.matrix(self.X[i:i + self.batch_size]), \n",
    "                   np.matrix(self.Y[i:i + self.batch_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "enhanced-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU:\n",
    "    def __init__(self, negative_slope = 0.01):\n",
    "        self.negative_slope = 0.01\n",
    "    \n",
    "    def __leaky_relu(self, matrix):\n",
    "        temp = np.matrix(matrix, dtype=float)\n",
    "        return np.maximum(temp, temp*self.negative_slope)\n",
    "\n",
    "    def derivative(self, matrix):\n",
    "        result = np.matrix(matrix, dtype=float)\n",
    "        result[result > 0] = 1\n",
    "        result[result < 0] = self.negative_slope\n",
    "        return result\n",
    "    \n",
    "    def __call__(self, matrix):\n",
    "        return self.__leaky_relu(matrix)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'LeakyReLU'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'LeakyReLU'\n",
    "    \n",
    "class Identical:\n",
    "    \n",
    "    def __init__(self): pass\n",
    "    \n",
    "    def __identical(self, matrix):\n",
    "        return matrix\n",
    "\n",
    "    def derivative(self, matrix):\n",
    "        return np.matrix(np.ones(matrix.shape))\n",
    "    \n",
    "    def __call__(self, matrix):\n",
    "        return self.__identical(matrix)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Identical'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Identical'\n",
    "    \n",
    "class Sigmoid:\n",
    "    \n",
    "    def __init__(self): pass\n",
    "\n",
    "    def __sigmoid(self, matrix):\n",
    "        return np.matrix(1/(1+np.exp(matrix)))\n",
    "\n",
    "    def derivative(self, matrix):\n",
    "        sigmoid = self.__sigmoid(matrix)\n",
    "        return np.multiply(sigmoid, (1- sigmoid))\n",
    "    \n",
    "    def __call__(self, matrix):\n",
    "        return self.__sigmoid(matrix)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Sigmoid'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Sigmoid'\n",
    "\n",
    "class Softmax:\n",
    "    \n",
    "    def __init__(self): pass\n",
    "\n",
    "    def val(self, matrix):\n",
    "        new_mat = matrix - np.max(matrix, axis = 0)\n",
    "        softmax = np.exp(-1 * new_mat) / np.sum(np.exp(-1 * new_mat), axis = 0)\n",
    "        return softmax\n",
    "\n",
    "    def derivative(self, matrix):\n",
    "        return np.matrix(np.ones(matrix.shape))\n",
    "    \n",
    "    def __call__(self, matrix):\n",
    "        return self.val(matrix)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Softmax'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Softmax'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "disciplinary-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    \n",
    "    def __init__(self): pass\n",
    "\n",
    "    def __mse(self, predicted_val, expected_val):\n",
    "        \n",
    "        assert np.shape(predicted_val) == np.shape(expected_val)\n",
    "        predicted_val, expected_val = np.matrix(predicted_val).copy(), np.matrix(expected_val).copy()\n",
    "        return np.mean(np.power(predicted_val - expected_val, 2), -1)\n",
    "\n",
    "    def derivative(self, predicted_val, expected_val):\n",
    "        assert np.shape(predicted_val) == np.shape(expected_val)\n",
    "        predicted_val, expected_val = np.matrix(predicted_val).copy(), np.matrix(expected_val).copy()\n",
    "        return 2*(predicted_val - expected_val)  \n",
    "    \n",
    "    def __call__(self, expected_val, predicted_val):\n",
    "        return self.__mse(predicted_val, expected_val)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'L2'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'L2'\n",
    "    \n",
    "class CrossEntropy:\n",
    "    \n",
    "    def __init__(self): pass\n",
    "    \n",
    "    def __crossentropy(self, predicted_val, expected_val):\n",
    "        assert np.shape(predicted_val) == np.shape(expected_val)\n",
    "        expected_val[expected_val < 0] = 1e-4\n",
    "        expected_log = np.log(expected_val)\n",
    "        return np.mean(np.multiply(predicted_val, expected_log)) * -1\n",
    "    \n",
    "    def derivative(self, predicted_val, expected_val):\n",
    "        assert np.shape(predicted_val) == np.shape(expected_val)\n",
    "        res = np.multiply(predicted_val, 1 / expected_val) * -1\n",
    "        return res\n",
    "    \n",
    "    def __call__(self, predicted_val, expected_val):\n",
    "        return self.__crossentropy(predicted_val, expected_val)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'CrossEntropy'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'CrossEntropy'\n",
    "\n",
    "class CrossEntropySoftmax:\n",
    "    \n",
    "    def __init__(self): pass\n",
    "\n",
    "    def val(self, predicted_val, expected_val):\n",
    "        assert np.shape(predicted_val)==np.shape(expected_val)\n",
    "        \n",
    "        predicted_val[predicted_val < 0] = 1e-4\n",
    "        predicted_log = np.log(predicted_val)\n",
    "        return np.mean(np.multiply(expected_val, predicted_log)) * -1\n",
    "        \n",
    "    def derivative(self, predicted_val, expected_val):\n",
    "        assert np.shape(predicted_val) == np.shape(expected_val)\n",
    "        cross_entropy_derivative = expected_val - predicted_val\n",
    "        return cross_entropy_derivative\n",
    "    \n",
    "    def __call__(self, predicted_val, expected_val):\n",
    "        return self.val(predicted_val, expected_val)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'CrossEntropyForSoftmax'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'CrossEntropyForSoftmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "electoral-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_weight(dim1, dim2, limit):\n",
    "    limits = {'LOW': 150, 'MEDIUM': 100, 'HIGH': 10}\n",
    "    return np.matrix(np.random.rand(dim1, dim2)) / 100\n",
    "\n",
    "def zero_weight(dim1, dim2):\n",
    "    return np.matrix(np.zeros((dim1, dim2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "digital-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    weight_initializer_dict = {'zero': zero_weight, 'random': random_weight}\n",
    "    def __init__(self, neurons, prev_layer_neurons, activation, weights_init = 'random', w_limit = 'MEDIUM'):\n",
    "        \n",
    "        assert type(weights_init) == str, 'Undefined activation function!'\n",
    "        assert weights_init in self.weight_initializer_dict, 'Undefined weight initialization function!'\n",
    "        \n",
    "        self.neurons = neurons\n",
    "        self.activation_function = activation\n",
    "        weight_initializer = self.weight_initializer_dict[weights_init]\n",
    "        self.w = weight_initializer(self.neurons, prev_layer_neurons, w_limit)\n",
    "        self.b = weight_initializer(self.neurons, 1, w_limit)\n",
    "        \n",
    "        \n",
    "        self.input = None\n",
    "        self.linear_output = None\n",
    "        self.activated_output = None\n",
    "        \n",
    "        self.last_weight_updating_value = 0\n",
    "        self.last_bias_updating_value = 0\n",
    "        self.dout_dl = None\n",
    "        \n",
    "    def forwardprop(self, _input):\n",
    "        \n",
    "        assert np.ndim(_input) == 2\n",
    "        assert self.weight.shape[1] == _input.shape[0]\n",
    "        \n",
    "        self.input = np.matrix(_input).astype(float)\n",
    "        self.linear_output = np.matmul(self.w, self.input) + self.b\n",
    "        self.activated_output = self.activation_function(self.linear_output)\n",
    "        self.dout_dl = self.activation_function.derivative(self.linear_output)\n",
    "        return self.activated_output\n",
    "    \n",
    "    \n",
    "    def backprop(self, backprop_tensor, lr, momentum):\n",
    "        assert np.ndim(backprop_tensor) == 2\n",
    "        assert backprop_tensor.shape[0] == self.dout_dl.shape[0]\n",
    "        assert backprop_tensor.shape[1] == self.dout_dl.shape[1]\n",
    "        \n",
    "        \n",
    "        backprop_tensor = np.matrix(backprop_tensor).astype(float)\n",
    "        backprop_tensor = np.multiply(backprop_tensor, self.dout_dl)\n",
    "        bias_updating_value = (1 - momentum) * np.sum(backprop_tensor, axis = 1) / backprop_tensor.shape[1] \\\n",
    "                            + momentum * self.last_bias_updating_value\n",
    "        weight_updating_value = (1 - momentum) * np.matmul(backprop_tensor, self.input.T) / backprop_tensor.shape[1] \\\n",
    "                            + momentum * self.last_weight_updating_value\n",
    "        backprop_tensor = np.matmul(self.w.T, backprop_tensor)\n",
    "\n",
    "        self.w -= lr * weight_updating_value\n",
    "        self.b -= lr * bias_updating_value\n",
    "        \n",
    "        self.last_weights_updating_value = weight_updating_value\n",
    "        self.last_bias_updating_value = bias_updating_value\n",
    "        \n",
    "        return backprop_tensor\n",
    "    \n",
    "    def get_number_of_neurons(self):\n",
    "        return self.neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chicken-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = None\n",
    "        self.layers = []\n",
    "        self.DEFAULT_LR = 1e-3\n",
    "        self.DEFAULT_MOMENTUM = 0\n",
    "        self.lr = None\n",
    "        self.momentum = None\n",
    "        self.loss = None\n",
    "        self.lr_AUTO = False\n",
    "        \n",
    "    def learning_rate(self, epoch):\n",
    "        return (390)/(epoch**(8/3)+103000)\n",
    "    \n",
    "    def add_layer(self, neurons, activation = LeakyReLU(), initial_weight = 'random', w_limit = \"MEDIUM\"):\n",
    "         \n",
    "        assert type(neurons) == int, \"Invalid number of neurons for the layer!\"\n",
    "        assert neurons > 0, \"Invalid number of neurons for the layer!\"\n",
    "        \n",
    "        if len(self.layers): prev_neurons = self.layers[-1].get_number_of_neurons()\n",
    "        else: prev_neurons = self.input_shape\n",
    "            \n",
    "        new_layer = Layer(neurons, prev_neurons, activation, initial_weight, w_limit)\n",
    "        self.layers.append(new_layer)\n",
    "        self.output_shape = self.layers[-1].get_number_of_neurons()\n",
    "        \n",
    "    \n",
    "    def set_training_param(self, loss = MSE(), **param):\n",
    "        assert self.layers, \"Uncomplete model!\"\n",
    "        self.loss = loss\n",
    "        self.lr = param['lr'] if 'lr' in param.keys() else self.DEFAULT_LR\n",
    "        self.momentum = param['momentum'] if 'momentum' in param.keys() else self.DEFAULT_MOMENTUM\n",
    "        if self.lr == 'AUTO': self.lr_AUTO = True\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def accuracy(self, X_test, X_train, Y_test, Y_train):\n",
    "        Y_pred_test = self.predict(X_test.T)\n",
    "        test_acc = np.mean(np.equal(np.argmax(Y_pred_test, axis = 0), np.argmax(Y_test.T, axis = 0)))\n",
    "        Y_pred_train = self.predict(X_train.T)\n",
    "        train_acc = np.mean(np.equal(np.argmax(Y_pred_train, axis = 0), np.argmax(Y_train.T, axis = 0)))\n",
    "        return test_acc, train_acc\n",
    "    \n",
    "    def get_network_info(self):\n",
    "        print(len(self.layers), 'layers:')\n",
    "        for layer in self.layers:\n",
    "            print(layer.get_number_of_neurons(), 'neurons.', 'activation function:', layer.activation_function)\n",
    "        print('Momentum:\\t' + str(self.momentum))\n",
    "        print('Loss Function:\\t', self.loss)\n",
    "        if self.lr_AUTO:\n",
    "            plt.plot(range(150), [self.learning_rate(i) for i in range(150)])\n",
    "            plt.title('Learning rate decay')\n",
    "            plt.show()\n",
    "        else: print('Learning rate:\\t' + str(self.lr))\n",
    "    \n",
    "    def epoch_log(self, i, train_loss, test_loss, train_acc, test_acc, acc):\n",
    "        print('-' * 15, 'EPOCH: ' + '#' + str(i), '-' * 15)\n",
    "        print('learning rate:\\t' + str(self.lr))\n",
    "        print('trin Loss:\\t' + str(round(train_loss, 4)) + '\\ttest Loss:\\t' + str(round(test_loss, 4)))\n",
    "        if acc: print('trin accuracy:\\t' + str(round(train_acc, 4)) + '\\ttest accuracy:\\t' + str(round(test_acc, 4)))\n",
    "        print()\n",
    "    \n",
    "    def forward(self, input_tensor):\n",
    "        assert type(self.output_shape) != None, \"Model is not compiled!\"\n",
    "        \n",
    "        output_tensor = input_tensor\n",
    "        for network_layer in self.layers:\n",
    "            output_tensor = network_layer.forwardprop(output_tensor)  \n",
    "        return output_tensor\n",
    "    \n",
    "    def optimize(self, backprop_tensor):\n",
    "        for network_layer in reversed(self.layers):\n",
    "            backprop_tensor = network_layer.backprop(backprop_tensor, self.lr, self.momentum)\n",
    "        \n",
    "    def fit(self, EPOCHS, trainloader, testloader, log = False, acc = False):\n",
    "        train_accs, test_accs, train_losses, test_losses = [], [], [], []\n",
    "        train_acc, test_acc = 0, 0\n",
    "        result = {}\n",
    "        now = time.time()\n",
    "        for i in range(EPOCHS):\n",
    "            if self.lr_AUTO: self.lr = round(self.learning_rate(i), 5)\n",
    "            train_loss = self.epoch_train(trainloader)\n",
    "            test_loss = self.epoch_test(testloader)\n",
    "            train_losses.append(train_loss), test_losses.append(test_loss)\n",
    "            if acc:\n",
    "                X_train, Y_train = trainloader.get_data()\n",
    "                X_test, Y_test = testloader.get_data()\n",
    "                test_acc, train_acc = self.accuracy(X_test, X_train, Y_test, Y_train)\n",
    "                train_accs.append(train_acc), test_accs.append(test_acc)\n",
    "            if log: self.epoch_log(i, train_loss, test_loss, train_acc, test_acc, acc)\n",
    "        print('-'*31)\n",
    "        print('--- Tooks ' + str() + '(s) to fit. ---')\n",
    "        print('-'*31)\n",
    "        result['train_loss'], result['test_loss'] = train_losses, test_losses\n",
    "        result['train_acc'], result['test_acc'] = train_accs, test_accs\n",
    "        return result\n",
    "    \n",
    "    def epoch_train(self, trainloader):\n",
    "        batch_losses = []\n",
    "        for x_train, y_train in trainloader:\n",
    "            batch_output = self.forward(x_train.T)\n",
    "            backprop_tensor = self.loss.derivative(batch_output, y_train.T)\n",
    "            self.optimize(backprop_tensor)\n",
    "            batch_loss = np.mean(self.loss(batch_output, y_train.T))\n",
    "            batch_losses.append(batch_loss)\n",
    "        return np.mean(batch_losses)\n",
    "    \n",
    "    def epoch_test(self, testloader):\n",
    "        batch_losses = []\n",
    "        for x_test, y_test in testloader:\n",
    "            test_output = self.forward(x_test.T)\n",
    "            batch_loss = np.mean(self.loss(test_output, y_test.T))\n",
    "            batch_losses.append(batch_loss)\n",
    "        return np.mean(batch_losses)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
